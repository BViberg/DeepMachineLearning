{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# HA2 - Recurrent Neural Networks for NLP\n",
    "\n",
    "This assignment is done in collaboration with Daniel Langkilde from Recorded Future. For questions, contact him on daniel@recordedfuture.com.\n",
    "\n",
    "### Named Entity Recognition\n",
    "The goal of this assignment is to build and train a model for [Named Entity Recognition](https://en.wikipedia.org/wiki/Named-entity_recognition). An important part of understanding natural language is being able to accurately locate and classify named entities. Named entities are nouns such as persons, organizations, locations etc. We will treat NER as a sequence labelling problem, and use Recurrent Neural Networks to solve it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This assignment is broken down into 6 main tasks:\n",
    "1. Loading the dataset and understanding it\n",
    "2. Computing the word embeddings\n",
    "3. Preprocessing the dataset\n",
    "4. Training a vanilla RNN for this task\n",
    "5. Training a deep LSTM for this task\n",
    "6. Evaluating the best model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dataset\n",
    "In this assignment we will a dataset called CoNLL-2002 Shared Task for Named Entity Recognition. It consists of 47 959 sentences, or 1 048 576 words including punctuation, with corresponding entity labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "conll_data = pd.read_csv(\"./conll_2002_ner_dataset.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Description of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Shape of dataset: \"+str(conll_data.shape))\n",
    "conll_data.head(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first column in the dataset is used to indicate when a new sentence begings. \n",
    "\n",
    "The second columns holds the word. \n",
    "\n",
    "The third column holds the Part-of-Speech tag. The POS tag describes the role of the word in the sentence (like adjective, noun, verb etc). More details are available here https://en.wikipedia.org/wiki/Part_of_speech \n",
    "\n",
    "The last column contains the Named Entity Tags. These are the tags we want to be able to predict for a given sentence. The tags in the dataset are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tags = list(conll_data.Tag.unique())\n",
    "tags.sort(key = lambda x: x[2:]) # sorting them to make it easier to read the list\n",
    "NUM_CLASSES = len(tags)\n",
    "print(\"Number of different tags: \"+str(NUM_CLASSES))\n",
    "tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "As you can see there are 9 different types represented in the dataset. The B- and I- prefixes indicate if the word is the beginning of an entity, or is inside an entity, putting the total number of classes at 17. For example \"Barack Obama\" would get the labels \"B-per I-per\". The entities represented are\n",
    "\n",
    "O&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- No type<br>\n",
    "Art&nbsp;&nbsp;&nbsp;&nbsp;- Artifact<br>\n",
    "Eve&nbsp;&nbsp;&nbsp;- Event<br>\n",
    "Geo &nbsp;- Geographical Entity<br>\n",
    "Gpe &nbsp;- Geopolitical Entity<br>\n",
    "Nat &nbsp;&nbsp;- Natural Phenomenon<br>\n",
    "Org &nbsp;&nbsp;- Organization<br>\n",
    "Per &nbsp;&nbsp;- Person<br>\n",
    "Tim &nbsp;&nbsp;- Time Expression<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Computing the word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The biggest challenge with NLP is that it's sparse. This is sometimes known as the curse of dimensionality. There are many ways to say the same thing, and the meaning of words is highly dependent on context. For computers the representation of words as discrete atoms is blunt. Two words have no inherent notion of similarity in their representation. \n",
    "\n",
    "\n",
    "To express this more formally, let's assume we represent words as vectors. The size of our vectors will be the size of our vocabulary. Each word can then be represented as a 1 in a specific position, and zero in all others. There is no meaningful way to compute the similarity between two words in this representation. Instead we've found a way to move from this symbolic representation to a distributed representation that inherently allows for similarity comparisons. \n",
    "\n",
    "We base this representation on the assumption that the meaning of a word is implicit in the context in which it appears. Or to paraphrase the famous linguist J.P. Firth ''You shall know a word by the company it keeps''. The idea behind word embeddings is to find a dense vector representation for words such that the cosine distance between two different words is directly proportional to the probability that those two words appear in the same context.\n",
    "\n",
    "We will now compute such dense vectors, or word embeddings as they are also known, for the words in our dataset. Rather than write everything ourselves we will use a package called gensim. There are some hyperparameters to choose for this, and default values are supplied."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we want to group the words by which sentence they occur in. We will drop the POS tag for this exercise.\n",
    "The goal is to get an ndarray with 47958 rows, each with two columns containing the array of words and their labels correspondingly. Fill in your code in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_conll_data(conll_data):\n",
    "    # complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw_data = read_conll_data(conll_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The required output is a numpy.ndarray containing one numpy.ndarray for each sentence. Inside each numpy.ndarray sentence are two numpy.ndarray containing the words and labels respectively. The first numpy.ndarray sentence should look like this:\n",
    "\n",
    "    array([ array(['Thousands', 'of', 'demonstrators', 'have', 'marched', 'through',\n",
    "           'London', 'to', 'protest', 'the', 'war', 'in', 'Iraq', 'and',\n",
    "           'demand', 'the', 'withdrawal', 'of', 'British', 'troops', 'from',\n",
    "           'that', 'country', '.'], \n",
    "          dtype='<U13'),\n",
    "           array(['O', 'O', 'O', 'O', 'O', 'O', 'B-geo', 'O', 'O', 'O', 'O', 'O',\n",
    "           'B-geo', 'O', 'O', 'O', 'O', 'O', 'B-gpe', 'O', 'O', 'O', 'O', 'O'], \n",
    "          dtype='<U5')], dtype=object)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll use gensim to compute the embeddings. Some hyperparameters are shown here for reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "VECTOR_SIZE = 100\n",
    "MIN_COUNT = 5\n",
    "MIN_ALPHA = 0.0001\n",
    "WINDOW = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given our raw dataset your task is to compute a gensim Word2Vec model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_sentences(data):\n",
    "    # complete\n",
    "    \n",
    "def compute_embeddings_model(data):\n",
    "    # complete\n",
    "\n",
    "sentences = get_sentences(raw_data)\n",
    "embeddings = compute_embeddings_model(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here `sentences` is of type `list` and embeddings is of `gensim.models.word2vec.Word2Vec`. If done correctly the following commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embeddings.most_similar('Germany')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embeddings.most_similar(positive=['Paris', 'France'], negative=['Berlin'], topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "should return something like (expect some variation due to the stochastic nature of t-SNE)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    [('France', 0.9594805240631104),\n",
    "     ('Britain', 0.8995000123977661),\n",
    "     ('Brazil', 0.8921124935150146),\n",
    "     ('Italy', 0.85428786277771),\n",
    "     ('Spain', 0.8505465984344482),\n",
    "     ('Canada', 0.8474853038787842),\n",
    "     ('Japan', 0.8394404053688049),\n",
    "     ('Argentina', 0.8286175727844238),\n",
    "     ('Netherlands', 0.8081429600715637),\n",
    "     ('Russia', 0.8042771220207214)]\n",
    "\n",
    "and\n",
    "\n",
    "    [('Germany', 0.7740741968154907),\n",
    "     ('Britain', 0.708018958568573),\n",
    "     ('Spain', 0.7058088779449463),\n",
    "     ('Japan', 0.7046102285385132),\n",
    "     ('Italy', 0.7018724679946899),\n",
    "     ('Asia', 0.6579268574714661),\n",
    "     ('Brazil', 0.6521944403648376),\n",
    "     ('Canada', 0.6436143517494202),\n",
    "     ('Vietnam', 0.6399591565132141),\n",
    "     ('Australia', 0.63737952709198)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the word Embeddings\n",
    "Before we move on to the neural networks, let's also plot the embeddings we've computed. To do this we will use a method called t-SNE. t-SNE allows us to find a nice projection of our high dimensional embeddings onto a two dimensional surface. The code is ready to run, no need to add anything to it.\n",
    "\n",
    "(this might take some minutes to run, depending on your hardware)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def compute_tsne(embeddings_model):\n",
    "    embeddings_model.init_sims(replace=True)\n",
    "    X = embeddings_model[embeddings_model.wv.vocab]\n",
    "    tsne = TSNE(n_components=2)\n",
    "    return tsne.fit_transform(X)\n",
    "\n",
    "tsne = compute_tsne(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save the resulting projection\n",
    "plt.rcParams[\"figure.figsize\"] = (50,50)\n",
    "plt.scatter(tsne[:, 0], tsne[:, 1])\n",
    "labels = list(embeddings.wv.vocab.keys())\n",
    "for label, x, y in zip(labels, tsne[:, 0], tsne[:, 1]):\n",
    "    plt.annotate(\n",
    "        label,\n",
    "        xy=(x, y), xytext=(-1, -1),\n",
    "        textcoords='offset points', ha='right', va='bottom')\n",
    "\n",
    "plt.savefig('tsne.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why is this visualization helpful? What sort of information does it provide us?\n",
    "\n",
    "**Your answer**: (fill in here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Preprocessing\n",
    "\n",
    "We will need to do some more preprocessing of our data for it to be useful to train an RNN. The following code defines some helpful variables for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NUM_SENTENCES = len(sentences)\n",
    "MAX_SENT_LENGTH = 0\n",
    "\n",
    "for sentence in sentences:\n",
    "    length = len(sentence)\n",
    "    if length > MAX_SENT_LENGTH: \n",
    "        MAX_SENT_LENGTH = length\n",
    "        \n",
    "print(\"Number of sentences: \"+str(NUM_SENTENCES))\n",
    "print(\"Maximum sentence length: \"+str(MAX_SENT_LENGTH))\n",
    "print(\"Number of target classes: \"+str(NUM_CLASSES))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 5 things we need to do before we are ready to train our Recurrent Neural Network. We will need to\n",
    "\n",
    "1. <b>Replace words with their embeddings</b><br>\n",
    "We've previously computed word embeddings, and we now need to transform our sequences of tokens into sequences of word vectors.\n",
    "<br>\n",
    "2. <b>One-hot encode targets</b><br>\n",
    "Each token can have any of our 17 classes. We want to represent the class of a token with a one-hot encoded vector.\n",
    "<br>\n",
    "3. <b>Zero-pad sentences and targets</b><br>\n",
    "We need all our sequences to be of equal length in order for the matrix algebra to work out. To achieve this we will zero pad each sentence with zero vectors, and each target with zero tags.\n",
    "<br>\n",
    "4. <b>Check dimensions of our data</b><br>\n",
    "It's a good practice to always check the dimensionality of your dataset before you start training your model.\n",
    "5. <b>Split into train and test set</b><br>\n",
    "Finally we want to split our dataset into a train and test set.\n",
    "\n",
    "### 3.1 Replace words with their embeddings\n",
    "We will now replace each word in our list of sentences with their word vector. If a word is not in our vocabulary we will sample its vector representation from a normal distribution instead (common practice for these problems)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_vector_for_unknown_word(dim):\n",
    "    unk_vec = 2*np.random.randn(dim)-1\n",
    "    norm_const = np.linalg.norm(unk_vec)\n",
    "    unk_vec /= norm_const\n",
    "    return unk_vec\n",
    "\n",
    "def replace_words_with_embeddings(sentences, embeddings):\n",
    "    # complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the list of sentences and our computed embeddings as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = replace_words_with_embeddings(sentences, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our output here should be of type List where each List entry is a List of word vectors. Word vectors will be of type `numpy.ndarray`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 One-hot encode targets\n",
    "Similar to the previous steps we will now replace our sequence of entity tags with vectors. Each entity tag will be one-hot encoded, which means that it will be represented by a vector with dimension equal to the number of classes. The entity type of a specific token will be represented by a 1, with all other classes 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_targets(data):\n",
    "    return list(map(lambda pair: pair[1], data))\n",
    "\n",
    "def one_hot_encode_targets(targets, tags):\n",
    "    # complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw_targets = get_targets(raw_data)\n",
    "targets = one_hot_encode_targets(raw_targets, tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the sentence case, raw_targets here will be of type List. Each entry in this list will be a numpy.ndarray of labels.\n",
    "\n",
    "targets will be of type List with each sentence represented by a numpy.ndarray of numpy.ndarrays holding 1s and 0s. The first sentence should look like this:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "array([[ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.],\n",
    "       [ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.],\n",
    "       [ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.],\n",
    "       [ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.],\n",
    "       [ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.],\n",
    "       [ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.],\n",
    "       [ 0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.],\n",
    "       [ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.],\n",
    "       [ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.],\n",
    "       [ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.],\n",
    "       [ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.],\n",
    "       [ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.],\n",
    "       [ 0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.],\n",
    "       [ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.],\n",
    "       [ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.],\n",
    "       [ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.],\n",
    "       [ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.],\n",
    "       [ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.],\n",
    "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.],\n",
    "       [ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.],\n",
    "       [ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.],\n",
    "       [ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.],\n",
    "       [ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.],\n",
    "       [ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
    "         0.,  0.,  0.,  0.]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Zero-pad sentences and targets\n",
    "We need all our sequences to be of equal length in order for the matrix algebra to work out. To achieve this we will zero-pad each sentence with zero vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def zero_pad_sentence(sentence):\n",
    "    # complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for sentence in data:\n",
    "    zero_pad_sentence(sentence)\n",
    "data = np.array(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we're finished with our preprocessing we transform our list of data points to a numpy.ndarray. That means we should now have data of type numpy.ndarray, where each sentence is represented by a numpy.ndarray of word vectors in the form of numpy.ndarrays. The shape should be `(47958, 104, 100)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will zero-pad our targets with the \"no entity\" type label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def zero_pad_targets(targets):\n",
    "    # complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "targets = np.array(zero_pad_targets(targets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to our data our targets should now be a numpy.ndarray of numpy.ndarray of numpy.ndarray. The shape should be `(47958, 104, 17)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Check data dimensions\n",
    "To ensure that we've succeeded in preprocessing all our data appropriately we will check some dimensionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NUM_SENT_OK = targets.shape[0] == data.shape[0]\n",
    "TARGET_LENGHT_OK = MAX_SENT_LENGTH == targets.shape[1]\n",
    "DATA_LENGTH_OK = MAX_SENT_LENGTH == data.shape[1]\n",
    "VECTOR_SIZE_OK = data.shape[2] == VECTOR_SIZE\n",
    "print(\"Target lenght ok: \"+str(TARGET_LENGHT_OK))\n",
    "print(\"Data lenght ok: \"+str(DATA_LENGTH_OK))\n",
    "print(\"Vector size ok: \"+str(VECTOR_SIZE_OK))\n",
    "print(\"Num sent ok: \"+str(NUM_SENT_OK))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The booleans should all be True to proceed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Split into train and test data\n",
    "We will split our dataset into a train and a test part. We will use 20% of the data to test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math \n",
    "TEST_FRACTION = 0.2\n",
    "NUM_TEST = math.ceil(NUM_SENTENCES * TEST_FRACTION)\n",
    "NUM_TRAIN = NUM_SENTENCES - NUM_TEST\n",
    "x_train = data[:NUM_TRAIN]\n",
    "y_train = targets[:NUM_TRAIN]\n",
    "x_test = data[NUM_TRAIN+1:]\n",
    "y_test = targets[NUM_TRAIN+1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training a simple RNN\n",
    "We are finally ready to train our first Recurrent Neural Network. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Design a single-layer vanilla RNN to predict the class of each word in the sentences.\n",
    "\n",
    "Hints:\n",
    "- Keras has a layer called `SimpleRNN`, for implementing this type of RNNs.\n",
    "- Maybe you want to take a look at the `TimeDistributed` layer too. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compile and train it. Motivate your choice of loss and metrics for this particular problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Motivations**: (fill in here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Testing new sentences\n",
    "Now that we have a model, let's check how it performs on some sentences. Create a function that takes as input a string, which will be the sentence we want to perform NER, and a Keras model for doing it. The output should be the predicted probabilities for each label, for each of the words in the provided sentence. More specifically, it should be a `numpy.ndarray` with shape (`num_words`, `num_classes`), where `num_words` is the number of words in the input sentence and `num_classes` is the number of classes we can predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def perform_ner(sentence, model):\n",
    "    # Complete:\n",
    "    \n",
    "    # Preprocess the sentence into what your model expects\n",
    "    \n",
    "    # Predict and return the result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test it on the following sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sentence = \"The constitution states that thou shalt not kill .\"\n",
    "prediction = perform_ner(sentence, simple_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make it easier to visualize the prediction probability mass function for each word, create a function that receives your prediction, and plots the pmf for each word separately. \n",
    "\n",
    "Requirements:\n",
    "- All plots should have titles showing the word associated to that pmf.\n",
    "- The labels for the x-axis should be the name of the tags, aligned vertically (i.e. `rotation='vertical'` when calling the `xticklabels` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def plot_prediction(sentence, prediction):\n",
    "    # complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_prediction(sentence, prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, write a function that takes your pmf predictions and converts it to hard predictions (i.e. returns a list with the most probable label for each word)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def decode_prediction(prediction):\n",
    "    # complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "decode_prediction(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we perform the same steps again, but for another sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentence2 = \"Barack Obama was the president of the United states .\"\n",
    "prediction2 = perform_ner(sentence2, simple_model)\n",
    "plot_prediction(sentence2, prediction)\n",
    "decode_prediction(prediction2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What can you observe from the output from these two sentences? \n",
    "\n",
    "**Your answer**: (fill in here)\n",
    "\n",
    "How was the word \"states\" classified in each sentence? Why do you think this happened?\n",
    "\n",
    "**Your answer**: (fill in here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Deep LSTM RNN\n",
    "As we stated before, distant dependencies between words are difficult to capture with vanilla RNNs. This is referred to as the vanishing/exploding gradient problem.\n",
    "\n",
    "Several different architectures for RNNs have been proposed to deal with the vanishing/exploding gradient problem. One of the most successful in recent years has been Long-Short Term Memory networks (LSTM). LSTM are capable of learning long-term dependencies. They were introduced in 1997, but have been popularized through the general increase in interest for deep learning. The difference between an LSTM and an RNN is the structure of the repeating module. The core idea is that LSTM have the ability to remove or add information to the cell state using structures called gates.\n",
    "\n",
    "We can further improve our model by introducing Long-Short Term Memory cells and adding multiple layers. Design a new model using LSTMs. This time, you're free to add more layers, and/or use other layer wrappers (like the `Bidirectional`, for instance)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compile and train your model. As before, motivate your choice of loss and metrics for this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Motivations**: (fill in here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test it on the same sentences as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test on new sentences you find relevant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluate your best model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which model performed best, the vanilla RNN or the one using LSTMs? How do you evaluate this?\n",
    "\n",
    "**Your answer**: (fill in here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the best model you obtained, evaluate its performance using the data from the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to contextualize the metrics you computed in the test set, it's helpful to have a baseline to compare to. Can you come up with a simple way to obtain a baseline performance for this problem? Explain.\n",
    "\n",
    "**Your asnwer**: (fill in here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you managed to solve the last question, implement it here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given your baseline performance, do you think your model performed well?\n",
    "\n",
    "**Your answer**: (fill in here)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
