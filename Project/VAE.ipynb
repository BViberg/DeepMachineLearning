{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 60, 2)\n",
      "(9000, 60, 2)\n"
     ]
    }
   ],
   "source": [
    "# Import data\n",
    "train = np.load('train_data_abs_coord_60.npy').astype(float)\n",
    "train=train[:50000,:,0:2]\n",
    "val = np.load('test_data_abs_coord_60.npy').astype(float)\n",
    "val=val[:9000,:,0:2]\n",
    "print((train.shape))\n",
    "print((val.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, LSTM, CuDNNLSTM, RepeatVector\n",
    "from keras.layers.core import Flatten, Dense, Dropout, Lambda\n",
    "from keras.optimizers import SGD, RMSprop, Adam\n",
    "from keras import objectives\n",
    "\n",
    "\n",
    "def create_lstm_vae(input_dim, \n",
    "    timesteps, \n",
    "    batch_size, \n",
    "    intermediate_dim, \n",
    "    latent_dim,\n",
    "    epsilon_std=1.):\n",
    "\n",
    "    \"\"\"\n",
    "    Creates an LSTM Variational Autoencoder (VAE). Returns VAE, Encoder, Generator. \n",
    "    # Arguments\n",
    "        input_dim: int.\n",
    "        timesteps: int, input timestep dimension.\n",
    "        batch_size: int.\n",
    "        intermediate_dim: int, output shape of LSTM. \n",
    "        latent_dim: int, latent z-layer shape. \n",
    "        epsilon_std: float, z-layer sigma.\n",
    "    # References\n",
    "        - [Building Autoencoders in Keras](https://blog.keras.io/building-autoencoders-in-keras.html)\n",
    "        - [Generating sentences from a continuous space](https://arxiv.org/abs/1511.06349)\n",
    "    \"\"\"\n",
    "    x = Input(shape=(timesteps, input_dim,))\n",
    "\n",
    "    # LSTM encoding\n",
    "    h = CuDNNLSTM(intermediate_dim)(x)\n",
    "\n",
    "    # VAE Z layer\n",
    "    z_mean = Dense(latent_dim)(h)\n",
    "    z_log_sigma = Dense(latent_dim)(h)\n",
    "    \n",
    "    def sampling(args):\n",
    "        z_mean, z_log_sigma = args\n",
    "        epsilon = K.random_normal(shape=(batch_size, latent_dim),\n",
    "                                  mean=0., stddev=epsilon_std)\n",
    "        return z_mean + K.exp(z_log_sigma) * epsilon\n",
    "\n",
    "    # note that \"output_shape\" isn't necessary with the TensorFlow backend\n",
    "    # so you could write `Lambda(sampling)([z_mean, z_log_sigma])`\n",
    "    z = Lambda(sampling, output_shape=(latent_dim,))([z_mean, z_log_sigma])\n",
    "    \n",
    "    # decoded LSTM layer\n",
    "    decoder_h = CuDNNLSTM(intermediate_dim, return_sequences=True)\n",
    "    decoder_mean = LSTM(input_dim, return_sequences=True,activation='linear')\n",
    "\n",
    "    h_decoded = RepeatVector(timesteps)(z)\n",
    "    h_decoded = decoder_h(h_decoded)\n",
    "\n",
    "    # decoded layer\n",
    "    x_decoded_mean = decoder_mean(h_decoded)\n",
    "    \n",
    "    # end-to-end autoencoder\n",
    "    vae = Model(x, x_decoded_mean)\n",
    "\n",
    "    # encoder, from inputs to latent space\n",
    "    encoder = Model(x, z_mean)\n",
    "\n",
    "    # generator, from latent space to reconstructed inputs\n",
    "    decoder_input = Input(shape=(latent_dim,))\n",
    "\n",
    "    _h_decoded = RepeatVector(timesteps)(decoder_input)\n",
    "    _h_decoded = decoder_h(_h_decoded)\n",
    "\n",
    "    _x_decoded_mean = decoder_mean(_h_decoded)\n",
    "    generator = Model(decoder_input, _x_decoded_mean)\n",
    "    \n",
    "    def vae_loss(x, x_decoded_mean):\n",
    "        xent_loss = objectives.mse(x, x_decoded_mean)\n",
    "        kl_loss = - 0.5 * K.mean(1 + z_log_sigma - K.square(z_mean) - K.exp(z_log_sigma))\n",
    "        loss = xent_loss + kl_loss\n",
    "        return loss\n",
    "    opt=Adam(lr=0.0001)\n",
    "    vae.compile(optimizer=opt, loss=vae_loss)\n",
    "    \n",
    "    return vae, encoder, generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 9000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 25s 491us/step - loss: 11.2633 - val_loss: 10.3863\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 23s 468us/step - loss: 9.6523 - val_loss: 8.7572\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 23s 468us/step - loss: 7.5207 - val_loss: 5.8804\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 23s 463us/step - loss: 5.5234 - val_loss: 4.9016\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 23s 465us/step - loss: 4.9693 - val_loss: 4.5332\n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 24s 471us/step - loss: 4.6360 - val_loss: 4.3259\n",
      "Epoch 7/20\n",
      "50000/50000 [==============================] - 23s 470us/step - loss: 4.2808 - val_loss: 4.4719\n",
      "Epoch 8/20\n",
      "50000/50000 [==============================] - 24s 476us/step - loss: 4.1775 - val_loss: 4.0336\n",
      "Epoch 9/20\n",
      "50000/50000 [==============================] - 23s 469us/step - loss: 4.7193 - val_loss: 3.9245\n",
      "Epoch 10/20\n",
      "50000/50000 [==============================] - 23s 467us/step - loss: 3.8824 - val_loss: 3.7459\n",
      "Epoch 11/20\n",
      "50000/50000 [==============================] - 24s 484us/step - loss: 3.8614 - val_loss: 3.4359\n",
      "Epoch 12/20\n",
      "50000/50000 [==============================] - 24s 473us/step - loss: 3.4314 - val_loss: 3.2349\n",
      "Epoch 13/20\n",
      "50000/50000 [==============================] - 24s 473us/step - loss: 3.3860 - val_loss: 3.1311\n",
      "Epoch 14/20\n",
      "50000/50000 [==============================] - 24s 475us/step - loss: 3.1432 - val_loss: 2.9886\n",
      "Epoch 15/20\n",
      "50000/50000 [==============================] - 24s 472us/step - loss: 3.0202 - val_loss: 2.8744\n",
      "Epoch 16/20\n",
      "50000/50000 [==============================] - 24s 472us/step - loss: 2.9584 - val_loss: 2.8402\n",
      "Epoch 17/20\n",
      "50000/50000 [==============================] - 24s 475us/step - loss: 2.8493 - val_loss: 2.7271\n",
      "Epoch 18/20\n",
      "50000/50000 [==============================] - 24s 474us/step - loss: 2.8305 - val_loss: 2.6395\n",
      "Epoch 19/20\n",
      "50000/50000 [==============================] - 24s 481us/step - loss: 2.6637 - val_loss: 2.5675\n",
      "Epoch 20/20\n",
      "50000/50000 [==============================] - 25s 490us/step - loss: 2.6071 - val_loss: 2.5143\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1906f85ea58>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.callbacks import TensorBoard,ModelCheckpoint    \n",
    "from time import time\n",
    "\n",
    "\n",
    "_,timesteps,input_dim=train.shape\n",
    "# Hyper paramters for network\n",
    "intermediate_dim = 512\n",
    "latent_dim = 128\n",
    "batch_size = 500\n",
    "vae, encoder, generator = create_lstm_vae(  input_dim, \n",
    "                                            timesteps, \n",
    "                                            batch_size, \n",
    "                                            intermediate_dim, \n",
    "                                            latent_dim,\n",
    "                                            epsilon_std=1.)\n",
    "# Hyper parmaeters for training\n",
    "batch_size = batch_size\n",
    "epochs = 20\n",
    "time=time()\n",
    "Name=\"tewst\"#f\"lat:{latent_dim}-inter:{intermediate_dim}-time{time}.\"\n",
    "tensorboard = TensorBoard(log_dir=\"logs/\"+Name)\n",
    "modelcheckpoint = ModelCheckpoint(\"model/\"+Name+\"epo:{epoch:02d}-val_loss{val_loss:.2f}.hdf5\",monitor='val_loss', verbose=0, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "callbacks =[tensorboard,modelcheckpoint]\n",
    "\n",
    "\n",
    "# Training with abs coord (intermediate_dim = 512) fairly succesfull up to 23 epochs. After investigation not true!\n",
    "                        #(latent_dim = 128)\n",
    "                        #(batch_size = 500)\n",
    "\n",
    "vae.fit( x = train, y = train, batch_size = batch_size , epochs = epochs, validation_data=(val, val),callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 9000 samples\n",
      "Epoch 1/40\n",
      "50000/50000 [==============================] - 23s 466us/step - loss: 1.7482 - val_loss: 1.7158\n",
      "Epoch 2/40\n",
      "50000/50000 [==============================] - 23s 457us/step - loss: 1.7946 - val_loss: 1.7033\n",
      "Epoch 3/40\n",
      "50000/50000 [==============================] - 23s 462us/step - loss: 1.7292 - val_loss: 1.7203\n",
      "Epoch 4/40\n",
      "50000/50000 [==============================] - 23s 460us/step - loss: 1.7229 - val_loss: 1.6896\n",
      "Epoch 5/40\n",
      "50000/50000 [==============================] - 23s 462us/step - loss: 1.7175 - val_loss: 1.7153\n",
      "Epoch 6/40\n",
      "50000/50000 [==============================] - 23s 462us/step - loss: 1.7249 - val_loss: 1.6730\n",
      "Epoch 7/40\n",
      "50000/50000 [==============================] - 23s 463us/step - loss: 1.7115 - val_loss: 1.7164\n",
      "Epoch 8/40\n",
      "50000/50000 [==============================] - 23s 465us/step - loss: 1.7180 - val_loss: 1.6622\n",
      "Epoch 9/40\n",
      "50000/50000 [==============================] - 23s 470us/step - loss: 1.6798 - val_loss: 1.6922\n",
      "Epoch 10/40\n",
      "50000/50000 [==============================] - 23s 465us/step - loss: 1.6795 - val_loss: 1.6507\n",
      "Epoch 11/40\n",
      "50000/50000 [==============================] - 23s 466us/step - loss: 1.7237 - val_loss: 1.7096\n",
      "Epoch 12/40\n",
      "50000/50000 [==============================] - 23s 465us/step - loss: 1.7368 - val_loss: 1.6636\n",
      "Epoch 13/40\n",
      "50000/50000 [==============================] - 23s 465us/step - loss: 1.6599 - val_loss: 1.6400\n",
      "Epoch 14/40\n",
      "50000/50000 [==============================] - 23s 467us/step - loss: 1.6484 - val_loss: 1.6259\n",
      "Epoch 15/40\n",
      "50000/50000 [==============================] - 23s 468us/step - loss: 1.6486 - val_loss: 1.6257\n",
      "Epoch 16/40\n",
      "50000/50000 [==============================] - 23s 464us/step - loss: 1.6402 - val_loss: 1.6084\n",
      "Epoch 17/40\n",
      "50000/50000 [==============================] - 23s 468us/step - loss: 1.6938 - val_loss: 1.6036\n",
      "Epoch 18/40\n",
      "50000/50000 [==============================] - 23s 467us/step - loss: 1.6272 - val_loss: 1.6001\n",
      "Epoch 19/40\n",
      "50000/50000 [==============================] - 23s 466us/step - loss: 1.6181 - val_loss: 1.5974\n",
      "Epoch 20/40\n",
      "50000/50000 [==============================] - 23s 468us/step - loss: 1.7094 - val_loss: 1.6041\n",
      "Epoch 21/40\n",
      "50000/50000 [==============================] - 23s 470us/step - loss: 1.6117 - val_loss: 1.5984\n",
      "Epoch 22/40\n",
      "50000/50000 [==============================] - 23s 468us/step - loss: 1.6073 - val_loss: 1.5868\n",
      "Epoch 23/40\n",
      "50000/50000 [==============================] - 23s 466us/step - loss: 1.5996 - val_loss: 1.5742\n",
      "Epoch 24/40\n",
      "50000/50000 [==============================] - 23s 468us/step - loss: 1.5901 - val_loss: 1.5761\n",
      "Epoch 25/40\n",
      "50000/50000 [==============================] - 23s 466us/step - loss: 1.6768 - val_loss: 1.5680\n",
      "Epoch 26/40\n",
      "50000/50000 [==============================] - 23s 468us/step - loss: 1.5837 - val_loss: 1.6037\n",
      "Epoch 27/40\n",
      "50000/50000 [==============================] - 23s 467us/step - loss: 1.5916 - val_loss: 1.5651\n",
      "Epoch 28/40\n",
      "50000/50000 [==============================] - 23s 467us/step - loss: 1.5690 - val_loss: 1.5493\n",
      "Epoch 29/40\n",
      "50000/50000 [==============================] - 23s 467us/step - loss: 1.5653 - val_loss: 1.5528\n",
      "Epoch 30/40\n",
      "50000/50000 [==============================] - 23s 468us/step - loss: 1.6065 - val_loss: 1.5640\n",
      "Epoch 31/40\n",
      "50000/50000 [==============================] - 23s 468us/step - loss: 1.5596 - val_loss: 1.5434\n",
      "Epoch 32/40\n",
      "50000/50000 [==============================] - 23s 468us/step - loss: 1.7030 - val_loss: 1.5607\n",
      "Epoch 33/40\n",
      "50000/50000 [==============================] - 23s 467us/step - loss: 1.5570 - val_loss: 1.5331\n",
      "Epoch 34/40\n",
      "50000/50000 [==============================] - 23s 468us/step - loss: 1.5445 - val_loss: 1.5295\n",
      "Epoch 35/40\n",
      "50000/50000 [==============================] - 23s 469us/step - loss: 1.5570 - val_loss: 1.5736\n",
      "Epoch 36/40\n",
      "50000/50000 [==============================] - 23s 468us/step - loss: 1.5386 - val_loss: 1.5275\n",
      "Epoch 37/40\n",
      "50000/50000 [==============================] - 23s 468us/step - loss: 1.5710 - val_loss: 1.7064\n",
      "Epoch 38/40\n",
      "50000/50000 [==============================] - 23s 468us/step - loss: 1.5808 - val_loss: 1.5145\n",
      "Epoch 39/40\n",
      "50000/50000 [==============================] - 23s 470us/step - loss: 1.5289 - val_loss: 1.5113\n",
      "Epoch 40/40\n",
      "50000/50000 [==============================] - 24s 471us/step - loss: 1.5263 - val_loss: 1.5084\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1921d1ea278>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vae.fit( x = train, y = train, batch_size = batch_size , epochs = 40, validation_data=(val, val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "import keras.losses\n",
    "# VAE Z layer\n",
    "z_mean = Dense(latent_dim)\n",
    "z_log_sigma = Dense(latent_dim)\n",
    "def vae_loss(x, x_decoded_mean):\n",
    "    xent_loss = objectives.mse(x, x_decoded_mean)\n",
    "    kl_loss = - 0.5 * K.mean(1 + z_log_sigma - K.square(z_mean) - K.exp(z_log_sigma))\n",
    "    loss = xent_loss + kl_loss\n",
    "    return loss\n",
    "keras.losses.vae_loss = vae_loss\n",
    "#vae=load_model('model/latent_dim128-intermediate_dim512-time1540355769.8443556.weights-23-3.41.hdf5', custom_objects={'vae_loss': vae_loss})\n",
    "predicted_val=vae.predict(val,batch_size=batch_size)\n",
    "predicted_train=vae.predict(train,batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAACFCAYAAABPL9NmAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4wLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvqOYd8AAADahJREFUeJzt3X2IXNd5x/HvT7O7bpWkOIk3wbUtyWlMqSlFNYuIUSlpZRJnKVEDLThZqKAOGyRC2//qsJC6lKUkfaWQldmmclTYNH1JTVyaJn4hISAUJ6tWcVRsN0orxY6FrVDSNghiafX0j3snO7s7o5nZubv3nLu/Dwwzc+bO7PPg1ePZ+5xzriICMzNrjl11B2BmZtVyYTczaxgXdjOzhnFhNzNrGBd2M7OGcWE3M2sYF3Yzs4ZxYTczaxgXdjOzhhmr44fecsstsW/fvjp+tJlZts6cOfO9iJjsd1wthX3fvn0sLy/X8aPNtsaxY7C4CCsr0GrB7CwsLNQdlTWMpIuDHFdLYTdrlGPH4Pjx1ecrK6vPXdytBj7HbjaqxcXhxs22mAu72ahWVoYbN9tiLuxmo2q1hhs322Iu7Gajmp0dbtxsi7mwm41qYQEOHVo7duiQG6dWGxd2s1EtLcHp02vHTp8uxs1q4MJuNqq5ObhyZe3YlSvFuFkNXNjNRvWd7ww3brbFXNjNRrVnz3DjZlvMhd1sVPPzMDGxdmxiohg3q4ELu1kVIm783GwbubCbjWpuDq5eXTt29aqbp1YbF3azUbl5aolxYTcblZunlhgXdrNRuXlqiXFhN6uCm6eWEBd2s1G5eWqJcWE3G5Wbp5YYF3azUbl5aomppLBLuiDpm5LOSvJVqm1nmZ+H3bvXju3e7eap1abKb+y/FBH7I2Kqws80S9/MDBw5snrFpFareD4zU29ctmP5VIzZqJaW4OTJ1WucrqwUz70fu9WkqsIewBOSzkjy9cBsZ/F+7JaYsYo+52BEvCzpLcCTkp6PiK90HlAW/FmAPW4qWZN4VowlppJv7BHxcnn/KvAYcKDLMYsRMRURU5OTk1X8WLM0eFaMJWbkwi7pdZLe0H4MvAs4N+rnmmXDWwpYYqo4FfNW4DFJ7c/7dER8oYLPNcuHtxSwhChq+AWcmpqK5WVPd7eG2LcPLl7cOL53L1y4sN3RWINJOjPIlHJPdzQblZunlhgXdrNRuXlqiXFht/wsLRWnP3btKu6XlrqPDXNsr/cfOwZjYyAV98eObTx2etrNU0uKz7FbXpaWYHZ27YKg8fGi8L722urY7t3Fsv6TJ/sf2+v9994LTz+9MYaxMbh2be37r19fXXnaHnv0UW8rYJUa9By7C7vlpVejsptWa22x3W5unlrF3Dy1ZhqmIVlnUQc3T602LuyWl2Eaku3dFuvi5qnVxIXd0tatUbl+7/Px8Y3Ny927i3Pxgxzb6/2HDnWPaWzdur7x8Y3/Exkfd/PUauPCbulqN0ovXixWcl68WDRDjxwpzl9Lxf2jj8KJE2vHFhdhYaG473dsr/c/9RQcPbp2n/WjR+FTn1p77Ac/uLGwFyuxzWrh5qmlK5cVnbnEadlz89Tyl8uKzlzitB3Dhd3SlcuKzlzitB3Dhd3SlctFonOJ03YMF3ZLx/oZMLCx+bm4mN5qTl/M2hLjwm5p6DYDZra8fO6FC8WS/QsX0iyWvpi1JcazYiwNOc8syTl2y4pnxVhecp5ZknPs1kgu7JaGnGeW5By7NVIlhV3S/ZJekHRe0kNVfKbtMPPzG5fqj43VP7NE2nhbr9esmLe/feNe7mbbYOTCLqkFfAJ4D3A38H5Jd4/6ubbDnDq1do9zKJ6fOlVPPNB7W4D14zMzG2fvtPdy72yoHj/u4m7bYuTmqaR7gYcj4t3l848ARMQf9nqPm6e2wdhY9212W62NBX+73Gi/l37/blLMx7K3nc3T24AXO56/VI6tD2hW0rKk5cuXL1fwY61Reu2dXvee6pvVtHwsK1UU9m5fazZ8nYmIxYiYioipycnJCn6sNUqvvdPr3lN9s5qWj2WlisL+EnBHx/PbgZcr+FzbSdqLkdZ75zu7X2Q6db3ymZ3tfeFss4qM9T+kr68Dd0m6E/gu8ADwgQo+13aShYXifnGxOF3RahVF/fTp1YtRd65G3Y4VqBHdz7MP0pfqls/sLBw8uPZi3Nudk+0Ilaw8lTQN/DnQAk5ExA3nqLl5agNp4orOJuZk22bQ5mkV39iJiM8Dn6/is8x+pIkrOpuYkyXHK08tXU1c0dnEnCw5LuyWribuc97EnCw5LuyWribuc97EnCw5LuyWribuc97EnCw53o/d0tXEGSRNzMm2jfdjt/w1cQZJE3Oy5LiwW7qaOIOkiTlZclzYLV29ZpBMT+e7JH9+HsbH146Nj3tWjFWqkgVKZluiPVNkbq44VbFnT1HUT57Me0n++m0KbrQ9sNkmuHlqecm9+Zh7/FYrN0+tmXJvPuYev2XBhd3yknvzMff4LQsu7JaX+XmYmFg7NjGRT0O1iQ1hS46bp5af9X2hlRX45Cfh6tXiecoN1aY2hC0pbp5aXno1H7vJpSHphqoNyM1Ta6Zhmoy5NCTdULWKubBbXoZpMubSkHRD1So2UmGX9LCk70o6W96mqwrMrKtuzceJidVtcDvHclnN6dWoVrEqvrH/WUTsL2++PJ5trZmZ4gLRe/cWKzb37oUHHyxmk3SqoXc0Eq9GtQqN1DyV9DDwg4j442He5+apVSr35mPu8du22c7m6YclPSvphKQ33iCgWUnLkpYvX75cwY81K+XefMw9fktO38Iu6SlJ57rcDgPHgZ8C9gOXgD/p9TkRsRgRUxExNTk5WVkCZtk3H3OP35LTt7BHxH0R8bNdbp+LiFciYiUirgN/CRzY+pDN1sn9AtG5x2/JGXVWzK0dT98HnBstHLNNuNEFopeW0l+q7wtcW8VG3VLg45L2AwFcAD40ckRmw+p1gWjIY6l+r/gPHkwrTsuGtxSw/PWaVdJqrRbLTqnNNvGsGBuQtxSwnaPX7JFuRf1Gx9fFs2KsYi7slr9es0fWr0btd3xdPCvGKubCbvnrNatkdrb73u2pzTaZn9+4cnbXrvTitGy4sFv+um0zsLhYNB/X95BS3Grg1Cm4fn3t2PXrxbjZJrh5as2VS1NybKx7P6DVgmvXtj8eS5abp2a5NCV7NXl7jZv14cJuzZVLU7JXk7fXuFkfLuzWXL0ufJ1aU7K9aGrQcbM+fDFra7YcmqcLC8X94mJx+qXVKop6e9xsSG6eWnPl0jw1G5Cbp2a5NE/NKubCbs2VS/PUrGIu7NZc3ufcdigXdmuuXitSvRWuNZxnxVizzcy4kNuOU8usGEmXgS7TFfq6BfhexeHUzTnloWk5NS0f2Bk57Y2IvheNrqWwb5ak5UGm+uTEOeWhaTk1LR9wTp18jt3MrGFc2M3MGia3wr5YdwBbwDnloWk5NS0fcE4/ktU5djMz6y+3b+xmZtaHC7uZWcNkUdgl/YGkZyWdlfSEpJ8sxyXpLySdL1+/p+5YByXpjyQ9X8b9mKSbO177SJnTC5LeXWecg5L065L+XdJ1SVPrXssunzZJ95dxn5f0UN3xbIakE5JelXSuY+xNkp6U9K3y/o11xjgsSXdI+pKk58rfu98ux7PNS9KPSfqapG+UOf1+OX6npGfKnP5W0kS/zyIikr8BP9Hx+LeAR8rH08C/AALeATxTd6xD5PQuYKx8/DHgY+Xju4FvADcBdwLfBlp1xztAPj8D/DTwZWCqYzzLfMrYW2W8bwMmyjzurjuuTeTxi8A9wLmOsY8DD5WPH2r//uVyA24F7ikfvwH4j/J3Ldu8yjr2+vLxOPBMWdf+DnigHH8EONrvs7L4xh4R/9vx9HVAu+N7GPjrKHwVuFnSrdse4CZExBMR0b5S8VeB28vHh4HPRMQPI+K/gPPAgTpiHEZEPBcRL3R5Kct8SgeA8xHxnxHxGvAZinyyEhFfAf573fBh4GT5+CTwq9sa1Igi4lJE/Gv5+P+A54DbyDivso79oHw6Xt4C+GXgH8rxgXLKorADSJqX9CIwA3y0HL4NeLHjsJfKsdz8JsVfHtCcnNpyzifn2Pt5a0RcgqJIAm+pOZ5Nk7QP+HmKb7hZ5yWpJeks8CrwJMVfjN/v+BI40O9gMoVd0lOSznW5HQaIiLmIuANYAj7cfluXj0pm/ma/nMpj5oBrFHlBwjkNkk+3t3UZSyKfAeQc+44g6fXAZ4HfWfeXfZYiYiUi9lP8BX+A4hTnhsP6fU4yuztGxH0DHvpp4J+B36P4v9cdHa/dDrxccWib1i8nSUeAXwEORXkCjYRzGuK/Uadk8xlAzrH384qkWyPiUnn68tW6AxqWpHGKor4UEf9YDmefF0BEfF/SlynOsd8saaz81j7Q72Ay39hvRNJdHU/fCzxfPn4c+I1ydsw7gP9p/xmWOkn3A78LvDcirnS89DjwgKSbJN0J3AV8rY4YK5JzPl8H7ipnJUwAD1Dk0wSPA0fKx0eAz9UYy9AkCfgr4LmI+NOOl7LNS9Jke3acpB8H7qPoHXwJ+LXysMFyqrsTPGC3+LPAOeBZ4J+A2zq6yJ+gOA/1TTpmY6R+o2givgicLW+PdLw2V+b0AvCeumMdMJ/3UXzD/SHwCvDFnPPpiH2aYsbFt4G5uuPZZA5/A1wCrpb/jR4E3gw8DXyrvH9T3XEOmdMvUJySeLbj39B0znkBPwf8W5nTOeCj5fjbKL4MnQf+Hrip32d5SwEzs4bJ4lSMmZkNzoXdzKxhXNjNzBrGhd3MrGFc2M3MGsaF3cysYVzYzcwa5v8B6ny2lhxzGjYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from keras.models import load_model\n",
    "b=4\n",
    "plt.close()\n",
    "\n",
    "predicted_abs=np.cumsum(predicted,axis=0)\n",
    "plt.subplot(211)\n",
    "#plt.plot(np.cumsum(predicted[b,:,0]), np.cumsum(predicted[b,:,1]), 'ro')\n",
    "plt.plot(predicted[b,:,0],predicted[b,:,1], 'ro')\n",
    "plt.axis('equal')\n",
    "plt.show()\n",
    "plt.subplot(212)\n",
    "#plt.plot(np.cumsum(train[b,:,0]), np.cumsum(train[b,:,1]), 'ro')\n",
    "plt.plot(val[b,:,0], val[b,:,1], 'ro')\n",
    "\n",
    "plt.axis('equal')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\bjorn\\\\Documents\\\\Repositories\\\\DeepMachineLearning\\\\Project'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "time=time()\n",
    "intermediate_dim = 512\n",
    "latent_dim = 64\n",
    "batch_size = 500\n",
    "Name=f\"latent_dim:{latent_dim}-intermediate_dim:{intermediate_dim}-batch_size:{batch_size}-time:{time}.\"\n",
    "print(Name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vae_loss(x, x_decoded_mean):\n",
    "    xent_loss = objectives.mse(x, x_decoded_mean)\n",
    "    kl_loss = - 0.5 * K.mean(1 + z_log_sigma - K.square(z_mean) - K.exp(z_log_sigma))\n",
    "    loss = xent_loss + kl_loss\n",
    "    return loss\n",
    "#vae=load_model('model/latent_dim128-intermediate_dim512-time1540355769.8443556.weights-23-3.41.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  1.  1.  1.  1.  0.  1.  1.  1.  0.  0.  0.\n",
      "  1.  1.  0.  1.  1.  1.  1.  0.  1.  1.  0.  1.  0.  1.  0.  0.  0.  0.\n",
      " -1.  0. -9.  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "print(train[b,:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "time=time()\n",
    "epochs =100\n",
    "Name=f\"lat{latent_dim}-inter{intermediate_dim}-epoch{epochs}.npy\"\n",
    "\n",
    "np.save(\"val_pred-abs-\"+Name,predicted_val)\n",
    "np.save(\"train_pred-abs-\"+Name,predicted_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
